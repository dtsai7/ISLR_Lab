---
title: "SL_lab_SVM"
author: "David Tsai"
date: "3/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## SVM
To demonstrate the SVM, it is easiedt to work in low dimensions.


# Linear SVM Classifier
Generate some data in two dimensions
```{r}
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1, ] = x[y == 1, ] + 1
plot(x, col = y + 3, pch = 19)
```

Load the package 'e1071' which contains the function 'svm'.
Have to specify a 'cost' parameter, which is a tuning parameter.
```{r}
library(e1071)
data.svm = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = data.svm, kernel = "linear", 
             cost = 10,  scale = FALSE) # not standardizing the variables
print(svmfit)
plot(svmfit, data.svm)
```

Make our own plot

1st. Make a grid of values for X1 and X2. Write a function to do it.
```{r}
make.grid = function(x, n = 75){
  grange = apply(x, 2, range)
  x1 = seq(from = grange[1, 1], to = grange[2, 1], length = n)
  x2 = seq(from = grange[1, 2], to = grange[2, 2], length = n)
  expand.grid(X1 = x1, X2 = x2)
}

xgrid = make.grid(x)
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index, ], pch = 5, cex = 2)
```


Use a formula to extract coefficients

Extract linear coefficients, and then using simple algebra, include the decision boundary and the two margins.
```{r}
beta = drop(t(svmfit$coefs) %*% x[svmfit$index, ])
beta0 = svmfit$rho

plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index, ], pch = 5, cex = 2)

abline(beta0/beta[2], -beta[1]/beta[2])
abline((beta0-1)/beta[2], -beta[1]/beta[2], lty = 2) # upper margin
abline((beta0+ 1)/beta[2], -beta[1]/beta[2], lty = 2) # lower margin
```



# Nonlinear SVM

```{r}
load(url("http://www-stat.stanford.edu/~tibs/ElemStatLearn/datasets/ESL.mixture.rda"))
names(ESL.mixture)
rm(x, y)
attach(ESL.mixture)
```

Plot the data and fit a nonlinear SVM, using a radial kernel.
```{r}
plot(x, col = y + 1)
data.nlin = data.frame(y = factor(y), x)
fit = svm(factor(y)~., data = data.nlin, scale = F, kernel = "radial",
          cost = 5)
```

Create a grid, as before and make predictions on the grid.
```{r}
xgrid = expand.grid(X1 = px1, X2 = px2)
ygrid = predict(fit, xgrid)
plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2)
points(x, col = y + 1, pch = 19)
```


Include the actual decision boundary on the plot by making use of the contour function.
```{r}
func = predict(fit, xgrid, decision.values = T)
func = attributes(func)$decision
xgrid = expand.grid(X1 = px1, X2 = px2)
ygrid = predict(fit, xgrid)

plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2)
points(x, col = y + 1, pch = 19)

contour(px1, px2, matrix(func, 69, 99), level = 0, add = T)
#bayes decision boundary
contour(px1, px2, matrix(prob, 69, 99), level = .5, add = T, col = "blue", lwd = 3) 
```



