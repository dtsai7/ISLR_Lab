---
title: "SL_lab_tree_based_methods"
author: "David Tsai"
date: "3/19/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Decision Trees
```{r}
require(ISLR)
require(tree)
attach(Carseats)

hist(Sales)
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
```

Fit a tree to these data

Exclude "Sales" from the right-hand side of the formula since the response is derived from it
```{r}
tree.carseats = tree(High ~ . - Sales, data = Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

For a detailed summary of the tree, print it:
```{r}
tree.carseats
```

Create a training and test set (250, 150) split of the 400 observations, grow the tree on the training set, and evaluate its performance on the test set
```{r}
set.seed(1011)
train = sample(1:nrow(Carseats), 250)
tree.carseats = tree(High ~ . - Sales, Carseats, subset = train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)

tree.pred = predict(tree.carseats, Carseats[-train,], type = "class")
with(Carseats[-train,], table(tree.pred, High))
```

Use cross-validation to prune the tree.
```{r}
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)
cv.carseats
plot(cv.carseats)
prune.carseats = prune.misclass(tree.carseats, best = 13)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

Evaluate the pruned tree on the test data.
```{r}
tree.pred = predict(prune.carseats, Carseats[-train,], type = "class")
with(Carseats[-train,], table(tree.pred, High))
```



## Random Forests
Random forests build lots of trees and then average them to reduce the variance.
```{r}
require(randomForest)
require(MASS)

set.seed(101)
dim(Boston)
train = sample(1:nrow(Boston), 300)
```

Fit a random forest and see how well it performs.
Use the response 'medv', the median housing value.
```{r}
rf.boston = randomForest(medv ~ ., data = Boston, subset = train) 
rf.boston
```
The MSR and % variance explained are based on OOB (out-of-bag) estimates.
The model reports that 'mtry = 4', which is the number of variables randomly chosen at each split.

```{r}
oob.err = double(13)
test.err = double(13)

for (mtry in 1:13){
  fit = randomForest(medv ~ ., data = Boston, subset = train, mtry = mtry, ntree = 400)
  oob.err[mtry] = fit$mse[400]
  pred = predict(fit, Boston[-train,])
  test.err[mtry] = with(Boston[-train,], mean((medv - pred) ^ 2))
  cat(mtry, " ")
}

matplot(1:mtry, cbind(test.err, oob.err), pch = 19, col = c("red", "blue"), 
        type = "b", ylab = "Mean Squared Error")

legend("topright", legend = c("OOB", "Test"), pch = 19, col = c("red", "blue"))
```



## Boosting
```{r}
require(gbm)
boost.boston = gbm(medv ~ ., data = Boston[train,], distribution = "gaussian", 
                   n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)

summary(boost.boston)
plot(boost.boston, i = "lstat")
plot(boost.boston, i = "rm")
```


Compute the test error as a function of the number of trees, and make a plot.
```{r}
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = Boston[-train,], n.trees = n.trees)
dim(predmat)

berr = with(Boston[-train,], apply((predmat - pred) ^ 2, 2, mean))
plot(n.trees, berr, pch = 19, ylab = "Mean Squared Error", xlab = "Number of Trees",
     main = "Boosting Test Error")
abline(h = min(test.err), col = "red")
```
