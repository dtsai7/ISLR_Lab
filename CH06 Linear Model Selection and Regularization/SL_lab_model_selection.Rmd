---
title: "SL_lab_model_selection"
author: "David Tsai"
date: "3/14/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
library(ISLR)
summary(Hitters)
```

## remove missing values
There are some missing values, so before proceeding we will remove them
```{r}
Hitters = na.omit(Hitters)
with(Hitters, sum(is.na(Salary)))
```


## Best Subset Selection
Use the package 'leaps' to evaluate all the best subset models.
```{r}
library(leaps)
regfit.full = regsubsets(Salary~., data = Hitters)
summary(regfit.full)
```

Increase best subsets to 19
```{r}
regfit.full = regsubsets(Salary~., data = Hitters, nvmax = 19)
reg.summary = summary(regfit.full)
names(reg.summary)
plot(reg.summary$cp, xlab = "Number of variables", ylab = "Cp")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], pch = 20, col = "red")
```

A plot method for the regsubset object
```{r}
plot(regfit.full, scale = "Cp")
coef(regfit.full, 10)
```

## Forward Stepwise Selection
Use the 'regsubsets' function but specify the method to forward
```{r}
regfit.fwd = regsubsets(Salary~., data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)
plot(regfit.fwd, scale = "Cp")
```

## Backward Stepwise Selection
Use the 'regsubsets' function but specify the method to backward
```{r}
regfit.bwd = regsubsets(Salary~., data = Hitters, nvmax = 19, method = "backward")
summary(regfit.bwd)
plot(regfit.bwd, scale = "Cp")
```

## Model Selection using a Validation Set
Make a training and validation set to choose a good subset model.
```{r}
dim(Hitters)
set.seed(1)
train = sample(seq(263), 180, replace = FALSE)
regfit.fwd = regsubsets(Salary~., data = Hitters[train,], nvmax = 19, method = "forward")
```
Make predictions on the observations not used for training.

```{r}
val.errors = rep(NA, 19)
x.test = model.matrix(Salary~., data = Hitters[-train,])
for (i in 1:19){
  coefi = coef(regfit.fwd, id = i)
  pred = x.test[,names(coefi)] %*% coefi
  val.errors[i] = mean((Hitters$Salary[-train] - pred) ^2)
}
plot(sqrt(val.errors), ylab = "Root MSE", ylim = c(300, 400), pch = 19, type = "b")
points(sqrt(regfit.fwd$rss[-1]/180), col = "blue", pch = 19, type = "b")
legend("topright", legend = c("Training", "Validation"), col = c("blue", "black"), 
       pch =19)
```

Create a predict method for 'regsubsets'
```{r}
predict.regsubsets = function(object, newdata, id,...){
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object, id = id)
  mat[,names(coefi)] %*% coefi
}
```

## Model selection by Cross-Validation
Use 10-fold cross-validation 
```{r}
set.seed(11)
folds = sample(rep(1:10, length = nrow(Hitters)))
table(folds)
cv.errors = matrix(NA, 10, 19)
for (k in 1:10){
  best.fit = regsubsets(Salary~., data = Hitters[folds != k,], nvmax = 19, 
                        method = "forward")
  for (i in 1:19){
    pred = predict(best.fit, Hitters[folds == k,], id = i)
    cv.errors[k, i] = mean((Hitters$Salary[folds == k] - pred) ^ 2)
  }
}
rmse.cv = sqrt(apply(cv.errors, 2, mean))
plot(rmse.cv, pch = 19, type ="b")
```

## Ridge Regression and Lasso Regression
```{r}
library(glmnet)
x = model.matrix(Salary~.-1, data = Hitters) #-1 for removing Salary
y = Hitters$Salary
```

Fit a ridge-regression model by calling 'glmnet' with 'alpha = 0'
('cv.glmnet' for cross-validation)
```{r}
fit.ridge = glmnet(x, y, alpha = 0)
plot(fit.ridge, xvar = "lambda", label = TRUE)

cv.ridge = cv.glmnet(x, y, alpha = 0)
plot(cv.ridge)
```

Now fit a lasso-regression model with alpha = 1
```{r}
fit.lasso = glmnet(x, y, alpha = 1)
plot(fit.lasso, xvar = "lambda", label = TRUE)
plot(fit.lasso, xvar = "dev", label = TRUE)

cv.lasso = cv.glmnet(x, y, alpha = 1)
plot(cv.lasso)
coef(cv.lasso)
```

Use earlier train/validation division to select the lambda for the lasso.
```{r}
lasso.tr = glmnet(x[train,], y[train])

pred = predict(lasso.tr, x[-train,])
dim(pred)

rmse = sqrt(apply((y[-train] - pred)^2, 2, mean))
plot(log(lasso.tr$lambda), rmse, type = "b", xlab = "Log(lambda)")
lam.best = lasso.tr$lambda[order(rmse)[1]]
coef(lasso.tr, s = lam.best)
```


